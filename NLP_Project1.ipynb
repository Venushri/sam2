{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdUV7SJz3k+aS2WfnxVSCq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Venushri/sam2/blob/main/NLP_Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROJECT ON BUILDING A LIBRARY USING PYTHON"
      ],
      "metadata": {
        "id": "WreXLbhsGuPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install and Import Dependencies"
      ],
      "metadata": {
        "id": "CQcogqZ2GY6r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elnBOL1gF5tu"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "import heapq\n",
        "import string\n",
        "import sentencepiece as spm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tokenize import WhitespaceTokenizer, TweetTokenizer\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import SyllableTokenizer\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "OPrUnWo2Hcs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from transformers import AutoTokenizer\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "ZfPA0ucWHdtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Download necessary data for NLTK"
      ],
      "metadata": {
        "id": "6Uc_UarbHCIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlpOkd1EI5SG",
        "outputId": "7acb42bb-90e7-4116-f617-b72dd701b752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Define Sample Text"
      ],
      "metadata": {
        "id": "GYpnNnaiHbnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Tokenization is an important step in NLP! It helps process text efficiently. Without tokenization, handling text would be much more difficult. Different tokenization methods serve different purposes.\"\n"
      ],
      "metadata": {
        "id": "HEBwbwcSHObo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenisation**\n",
        "Tokenization is the process of breaking down text into smaller components, typically words, phrases, or subwords, to facilitate analysis or processing by a machine."
      ],
      "metadata": {
        "id": "viL8B08m0klC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Word Tokenization"
      ],
      "metadata": {
        "id": "HPaOBz3SJJW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokenization:\", word_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjC3UJGYHfJm",
        "outputId": "3a0ad78d-bd39-469d-c1aa-8417a83d68b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization: ['Tokenization', 'is', 'an', 'important', 'step', 'in', 'NLP', '!', 'It', 'helps', 'process', 'text', 'efficiently', '.', 'Without', 'tokenization', ',', 'handling', 'text', 'would', 'be', 'much', 'more', 'difficult', '.', 'Different', 'tokenization', 'methods', 'serve', 'different', 'purposes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " b. Sentence Tokenization"
      ],
      "metadata": {
        "id": "a7JJqy6WJM5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\", sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ED-gkOFIXSN",
        "outputId": "671a4ebf-0b18-4a31-8bd3-6c1af0fa9f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization: ['Tokenization is an important step in NLP!', 'It helps process text efficiently.', 'Without tokenization, handling text would be much more difficult.', 'Different tokenization methods serve different purposes.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Whitespace **Tokenization**"
      ],
      "metadata": {
        "id": "YiTG9eEgJSiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whitespace_tokens = WhitespaceTokenizer().tokenize(text)\n",
        "print(\"Whitespace Tokenization:\", whitespace_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNvp2-MOIYCH",
        "outputId": "02fef275-d160-4ae1-fcc0-c1383069249f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenization: ['Tokenization', 'is', 'an', 'important', 'step', 'in', 'NLP!', 'It', 'helps', 'process', 'text', 'efficiently.', 'Without', 'tokenization,', 'handling', 'text', 'would', 'be', 'much', 'more', 'difficult.', 'Different', 'tokenization', 'methods', 'serve', 'different', 'purposes.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**d**. Character Tokenization"
      ],
      "metadata": {
        "id": "irhVnr8JJqY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_tokens = list(text)\n",
        "print(\"Character Tokenization:\", char_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn-2PKU-IbJz",
        "outputId": "20742de8-03d3-4d98-8ea6-a7c9718775f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character Tokenization: ['T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 'a', 'n', ' ', 'i', 'm', 'p', 'o', 'r', 't', 'a', 'n', 't', ' ', 's', 't', 'e', 'p', ' ', 'i', 'n', ' ', 'N', 'L', 'P', '!', ' ', 'I', 't', ' ', 'h', 'e', 'l', 'p', 's', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', ' ', 't', 'e', 'x', 't', ' ', 'e', 'f', 'f', 'i', 'c', 'i', 'e', 'n', 't', 'l', 'y', '.', ' ', 'W', 'i', 't', 'h', 'o', 'u', 't', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ',', ' ', 'h', 'a', 'n', 'd', 'l', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 'm', 'u', 'c', 'h', ' ', 'm', 'o', 'r', 'e', ' ', 'd', 'i', 'f', 'f', 'i', 'c', 'u', 'l', 't', '.', ' ', 'D', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'm', 'e', 't', 'h', 'o', 'd', 's', ' ', 's', 'e', 'r', 'v', 'e', ' ', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 'p', 'u', 'r', 'p', 'o', 's', 'e', 's', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "e. Subword Tokenization (Simulating Byte Pair Encoding using regex)"
      ],
      "metadata": {
        "id": "U_3oyzo4J0cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regex_tokens = regexp_tokenize(text, pattern='\\w+|\\$[\\d\\.]+|\\S')\n",
        "print(\"Regex-Based Tokenization:\", regex_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yWaViBbIeH1",
        "outputId": "943bb351-7644-45ca-c936-f2b2d290b2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regex-Based Tokenization: ['Tokenization', 'is', 'an', 'important', 'step', 'in', 'NLP', '!', 'It', 'helps', 'process', 'text', 'efficiently', '.', 'Without', 'tokenization', ',', 'handling', 'text', 'would', 'be', 'much', 'more', 'difficult', '.', 'Different', 'tokenization', 'methods', 'serve', 'different', 'purposes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " f. Tweet Tokenization (Handling social media text better)"
      ],
      "metadata": {
        "id": "8nZslSD_Kv2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hey @user! Check this out: https://example.com 😊 #NLP #AI\"\n"
      ],
      "metadata": {
        "id": "5I7L4Qj-Lsy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text1)\n",
        "print(\"Tweet Tokenization:\", tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsLfwVusIga7",
        "outputId": "8e3d18b9-8b7b-49aa-f85a-7bba010c4851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet Tokenization: ['Hey', '@user', '!', 'Check', 'this', 'out', ':', 'https://example.com', '😊', '#NLP', '#AI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Procesing**\n",
        " Text processing refers to the manipulation, transformation, and analysis of textual data to prepare it for further use in tasks like natural language processing (NLP), machine learning, and data analysis."
      ],
      "metadata": {
        "id": "RGAGOvdSMNes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Convert to lowercase"
      ],
      "metadata": {
        "id": "gmyOoTg5VXw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_lower = text.lower()\n",
        "print(\"Lowercased Text:\", text_lower)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMDqOkzvUk3p",
        "outputId": "d22f040f-9f7e-4738-e71e-96a69d74e69f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercased Text: tokenization is an important step in nlp! it helps process text efficiently. without tokenization, handling text would be much more difficult. different tokenization methods serve different purposes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Remove punctuation"
      ],
      "metadata": {
        "id": "9j1pjC2RVfgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_no_punct = text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(\"Text without Punctuation:\", text_no_punct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqI8k-oNUpQW",
        "outputId": "ad01f8fe-bade-4986-f691-e574fa79001f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without Punctuation: Tokenization is an important step in NLP It helps process text efficiently Without tokenization handling text would be much more difficult Different tokenization methods serve different purposes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Remove stopwords"
      ],
      "metadata": {
        "id": "cT-I11h3Vi23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in word_tokenize(text_no_punct) if word.lower() not in stop_words]\n",
        "print(\"Text without Stopwords:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zhfDYEuUssh",
        "outputId": "68b3ec9d-5358-4df5-97f0-163b7c138340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without Stopwords: ['Tokenization', 'important', 'step', 'NLP', 'helps', 'process', 'text', 'efficiently', 'Without', 'tokenization', 'handling', 'text', 'would', 'much', 'difficult', 'Different', 'tokenization', 'methods', 'serve', 'different', 'purposes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "d. Stemming (reducing words to their root form)"
      ],
      "metadata": {
        "id": "cqGDf73fVnEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "print(\"Stemmed Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK9oJt6vUvgq",
        "outputId": "bbc21a9c-c8b0-4bb0-ce71-c05cf895cdb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['token', 'import', 'step', 'nlp', 'help', 'process', 'text', 'effici', 'without', 'token', 'handl', 'text', 'would', 'much', 'difficult', 'differ', 'token', 'method', 'serv', 'differ', 'purpos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "e. Lemmatization (getting base words, better than stemming)"
      ],
      "metadata": {
        "id": "Nlo5-aKUVwfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEN0W3XuU2f0",
        "outputId": "81748f48-f231-4a17-8d0b-516fc01aa5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['Tokenization', 'important', 'step', 'NLP', 'help', 'process', 'text', 'efficiently', 'Without', 'tokenization', 'handling', 'text', 'would', 'much', 'difficult', 'Different', 'tokenization', 'method', 'serve', 'different', 'purpose']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frequency Analysis\n",
        "Frequency analysis is a fundamental step in Natural Language Processing (NLP) and is useful for understanding the distribution of words, characters, or n-grams in a text dataset."
      ],
      "metadata": {
        "id": "_qC6fYTwXUM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Tokenization is an important step in NLP! It helps process text efficiently. Without tokenization, handling text would be much more difficult. Different tokenization methods serve different purposes.\"\n"
      ],
      "metadata": {
        "id": "jMDauZhQZ5QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_text_frequencies(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Count word frequencies\n",
        "    word_freq = Counter(words)\n",
        "\n",
        "    # Count character frequencies (excluding spaces)\n",
        "    char_freq = Counter(text.replace(\" \", \"\"))\n",
        "\n",
        "    return word_freq, char_freq\n",
        "\n",
        "word_counts, char_counts = count_text_frequencies(text)\n",
        "\n",
        "print(\"\\nWord Frequency:\")\n",
        "for word, freq in word_counts.items():\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "print(\"\\nCharacter Frequency:\")\n",
        "for char, freq in char_counts.items():\n",
        "    print(f\"{char}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMyu7nvTXcVy",
        "outputId": "1b134036-d4e3-49a1-8c6e-3c17b5ba63c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Frequency:\n",
            "tokenization: 3\n",
            "is: 1\n",
            "an: 1\n",
            "important: 1\n",
            "step: 1\n",
            "in: 1\n",
            "nlp: 1\n",
            "it: 1\n",
            "helps: 1\n",
            "process: 1\n",
            "text: 2\n",
            "efficiently: 1\n",
            "without: 1\n",
            "handling: 1\n",
            "would: 1\n",
            "be: 1\n",
            "much: 1\n",
            "more: 1\n",
            "difficult: 1\n",
            "different: 2\n",
            "methods: 1\n",
            "serve: 1\n",
            "purposes: 1\n",
            "\n",
            "Character Frequency:\n",
            "t: 21\n",
            "o: 13\n",
            "k: 3\n",
            "e: 20\n",
            "n: 15\n",
            "i: 18\n",
            "z: 3\n",
            "a: 6\n",
            "s: 9\n",
            "m: 4\n",
            "p: 7\n",
            "r: 7\n",
            "l: 6\n",
            "h: 5\n",
            "c: 4\n",
            "x: 2\n",
            "f: 8\n",
            "y: 1\n",
            "w: 2\n",
            "u: 5\n",
            "d: 6\n",
            "g: 1\n",
            "b: 1\n",
            "v: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Various NLP techniques that we can integrate into other projects.\n",
        "This script implements various NLP techniques using nltk, spacy, and textblob. We can integrate these functions into other projects for text processing, sentiment analysis, and summarization."
      ],
      "metadata": {
        "id": "N2Ut7Wr023W9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load Spacy Model"
      ],
      "metadata": {
        "id": "ZXnWcfmN5lem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    sentences = sent_tokenize(text)\n",
        "    return words, sentences\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(text)\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "def named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "def sentiment_analysis(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.polarity\n",
        "\n",
        "def text_summarization(text, num_sentences=3):\n",
        "    sentences = sent_tokenize(text)\n",
        "    word_frequencies = Counter(word_tokenize(text.lower()))\n",
        "    max_freq = max(word_frequencies.values())\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word] /= max_freq\n",
        "    sentence_scores = {sent: sum(word_frequencies.get(word, 0) for word in word_tokenize(sent.lower())) for sent in sentences}\n",
        "    summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
        "    return ' '.join(summary_sentences)\n",
        "\n",
        "def keyword_extraction(text, num_keywords=5):\n",
        "    words = word_tokenize(text.lower())\n",
        "    words = [word for word in words if word.isalnum() and word not in stopwords.words('english')]\n",
        "    word_freq = Counter(words)\n",
        "    return [word for word, freq in word_freq.most_common(num_keywords)]\n"
      ],
      "metadata": {
        "id": "vR3zWGLc5YEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Example for Usage"
      ],
      "metadata": {
        "id": "Xcs5JKTO5uCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    sample_text = (\"In recent years, artificial intelligence has made significant advancements, \"\n",
        "                   \"impacting various industries including healthcare, finance, and transportation. \"\n",
        "                   \"Many companies are investing heavily in AI research to develop smarter systems. \"\n",
        "                   \"For example, Tesla is working on self-driving cars, and OpenAI has been \"\n",
        "                   \"developing language models like ChatGPT to enhance human-computer interactions. \"\n",
        "                   \"Experts believe that AI will continue to evolve, bringing both opportunities and challenges.\")\n",
        "    sample_text2 = (\"The project was a complete disaster,\"\n",
        "                    \"filled with endless delays, mismanagement, and poor execution,\"\n",
        "                    \"leading to frustration and disappointment among everyone involved.\")"
      ],
      "metadata": {
        "id": "UZv2cDZs5c_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Output for a positve statement."
      ],
      "metadata": {
        "id": "vsWSGz6eGlWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"Tokenization:\", tokenize_text(sample_text))\n",
        "    print(\"Lemmatization:\", lemmatize_text(sample_text))\n",
        "    print(\"NER:\", named_entity_recognition(sample_text))\n",
        "    print(\"Sentiment:\", sentiment_analysis(sample_text))\n",
        "    print(\"Summarization:\", text_summarization(sample_text))\n",
        "    print(\"Keywords:\", keyword_extraction(sample_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWSshwxj5g9Y",
        "outputId": "da71b26f-fc19-40ee-c713-7a85676da839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization: (['In', 'recent', 'years', ',', 'artificial', 'intelligence', 'has', 'made', 'significant', 'advancements', ',', 'impacting', 'various', 'industries', 'including', 'healthcare', ',', 'finance', ',', 'and', 'transportation', '.', 'Many', 'companies', 'are', 'investing', 'heavily', 'in', 'AI', 'research', 'to', 'develop', 'smarter', 'systems', '.', 'For', 'example', ',', 'Tesla', 'is', 'working', 'on', 'self-driving', 'cars', ',', 'and', 'OpenAI', 'has', 'been', 'developing', 'language', 'models', 'like', 'ChatGPT', 'to', 'enhance', 'human-computer', 'interactions', '.', 'Experts', 'believe', 'that', 'AI', 'will', 'continue', 'to', 'evolve', ',', 'bringing', 'both', 'opportunities', 'and', 'challenges', '.'], ['In recent years, artificial intelligence has made significant advancements, impacting various industries including healthcare, finance, and transportation.', 'Many companies are investing heavily in AI research to develop smarter systems.', 'For example, Tesla is working on self-driving cars, and OpenAI has been developing language models like ChatGPT to enhance human-computer interactions.', 'Experts believe that AI will continue to evolve, bringing both opportunities and challenges.'])\n",
            "Lemmatization: ['In', 'recent', 'year', ',', 'artificial', 'intelligence', 'ha', 'made', 'significant', 'advancement', ',', 'impacting', 'various', 'industry', 'including', 'healthcare', ',', 'finance', ',', 'and', 'transportation', '.', 'Many', 'company', 'are', 'investing', 'heavily', 'in', 'AI', 'research', 'to', 'develop', 'smarter', 'system', '.', 'For', 'example', ',', 'Tesla', 'is', 'working', 'on', 'self-driving', 'car', ',', 'and', 'OpenAI', 'ha', 'been', 'developing', 'language', 'model', 'like', 'ChatGPT', 'to', 'enhance', 'human-computer', 'interaction', '.', 'Experts', 'believe', 'that', 'AI', 'will', 'continue', 'to', 'evolve', ',', 'bringing', 'both', 'opportunity', 'and', 'challenge', '.']\n",
            "NER: [('recent years', 'DATE'), ('AI', 'GPE'), ('Tesla', 'ORG'), ('OpenAI', 'GPE'), ('AI', 'GPE')]\n",
            "Sentiment: 0.012500000000000002\n",
            "Summarization: In recent years, artificial intelligence has made significant advancements, impacting various industries including healthcare, finance, and transportation. For example, Tesla is working on self-driving cars, and OpenAI has been developing language models like ChatGPT to enhance human-computer interactions. Experts believe that AI will continue to evolve, bringing both opportunities and challenges.\n",
            "Keywords: ['ai', 'recent', 'years', 'artificial', 'intelligence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Output for a negative statement."
      ],
      "metadata": {
        "id": "iHqZ7Zt2Gr04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"Tokenization:\", tokenize_text(sample_text2))\n",
        "    print(\"Lemmatization:\", lemmatize_text(sample_text2))\n",
        "    print(\"NER:\", named_entity_recognition(sample_text2))\n",
        "    print(\"Sentiment:\", sentiment_analysis(sample_text2))\n",
        "    print(\"Summarization:\", text_summarization(sample_text2))\n",
        "    print(\"Keywords:\", keyword_extraction(sample_text2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJh-B8vQCC5Z",
        "outputId": "466fc097-1630-45f1-c29b-fe828c000cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization: (['The', 'project', 'was', 'a', 'complete', 'disaster', ',', 'filled', 'with', 'endless', 'delays', ',', 'mismanagement', ',', 'and', 'poor', 'execution', ',', 'leading', 'to', 'frustration', 'and', 'disappointment', 'among', 'everyone', 'involved', '.'], ['The project was a complete disaster,filled with endless delays, mismanagement, and poor execution,leading to frustration and disappointment among everyone involved.'])\n",
            "Lemmatization: ['The', 'project', 'wa', 'a', 'complete', 'disaster', ',', 'filled', 'with', 'endless', 'delay', ',', 'mismanagement', ',', 'and', 'poor', 'execution', ',', 'leading', 'to', 'frustration', 'and', 'disappointment', 'among', 'everyone', 'involved', '.']\n",
            "NER: []\n",
            "Sentiment: -0.25625\n",
            "Summarization: The project was a complete disaster,filled with endless delays, mismanagement, and poor execution,leading to frustration and disappointment among everyone involved.\n",
            "Keywords: ['project', 'complete', 'disaster', 'filled', 'endless']\n"
          ]
        }
      ]
    }
  ]
}